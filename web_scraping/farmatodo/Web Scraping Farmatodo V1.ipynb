{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555cd01a-0fad-4425-a2a9-fe8c573315cf",
   "metadata": {},
   "source": [
    "# Web Scraping www.farmatodo.com.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ba2ff8bf-f32a-4383-bc45-5ed17d16e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\Fsalinas\\Documents\\GitHub\\boticarios')\n",
    "time.sleep(2)\n",
    "os.chdir('./paquetes')\n",
    "from connpostgres import conn2\n",
    "from runSQL import RunDML, RunDDL\n",
    "time.sleep(2)\n",
    "os.chdir(r'C:\\Users\\Fsalinas\\Documents\\GitHub\\boticarios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "62543d78-2087-49b4-97d7-cc79bed67806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib.request\n",
    "from contextlib import closing\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a68b99-a857-46f0-b7ee-10d0183ded0e",
   "metadata": {},
   "source": [
    "_________\n",
    "### Web Crawler Categorias\n",
    "Extraé urls de categorias que contienen las url de los productos para posteriormente extraer información de producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "39487f01-407a-41e6-8b92-b4804d2b73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rutas a urls y archivos\n",
    "chromedriver = './web_scraping/chromedriver/chromedriver.exe'\n",
    "url_principal = 'https://www.farmatodo.com.co'\n",
    "\n",
    "# Define si el navegador estará visible durante el proceso\n",
    "hide_browser = False\n",
    "\n",
    "# Aplica opciones al navegador para evitar cargar recursos innecesarios\n",
    "options = Options()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "if hide_browser: options.add_argument('--headless')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_experimental_option('prefs',{'profile.managed_default_content_setings.images':2})\n",
    "\n",
    "with closing(Chrome(executable_path = chromedriver, options=options)) as navegador:\n",
    "    # navegador = Chrome(executable_path = chromedriver, options=options)\n",
    "    navegador.get(url_principal)\n",
    "    soup = BeautifulSoup(navegador.page_source, 'html.parser')\n",
    "\n",
    "    cat_urls = [x.get_attribute_list('href')[0].replace(url_principal, '') for x in soup.find_all('a') if x.get_attribute_list('href')[0]!=None and '/categorias' in x.get_attribute_list('href')[0]]\n",
    "    cat_urls = list(dict.fromkeys(cat_urls))\n",
    "\n",
    "    # Guarda la lista de URLs en un archivo csv\n",
    "    with open('./web_scraping/data/cat_urls_farmatodo.csv', 'w+') as f:\n",
    "        f.write('\\n'.join(cat_urls))\n",
    "\n",
    "    print(f'Número de urls de categorias: {len(cat_urls)}')\n",
    "    cat_urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568fa75-e4b2-4ad8-b013-12613cfaa6ce",
   "metadata": {},
   "source": [
    "_________\n",
    "### Web Crawler Productos\n",
    "Extraé urls de productos a partir de las URLs de categorias del archvio csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7a45feb9-590d-4a8f-a5cc-c74feeea8026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rutas a urls y archivos\n",
    "chromedriver = './web_scraping/chromedriver/chromedriver.exe'\n",
    "url_principal = 'https://www.farmatodo.com.co'\n",
    "\n",
    "# Define si el navegador estará visible durante el proceso\n",
    "hide_browser = False\n",
    "\n",
    "# Aplica opciones al navegador para evitar cargar recursos innecesarios\n",
    "options = Options()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "if hide_browser: options.add_argument('--headless')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_experimental_option('prefs',{'profile.managed_default_content_setings.images':2})\n",
    "\n",
    "# Leer el archivo con las url de categorias\n",
    "with open('./web_scraping/data/cat_urls_farmatodo.csv', 'r') as f:\n",
    "    cat_urls = f.read()\n",
    "cat_urls = cat_urls.split('\\n')\n",
    "\n",
    "def obtiene_data_producto(soup):\n",
    "    url = '/producto/' + str(soup).split('data-cuf=\"')[1].split('\"')[0]\n",
    "    try:\n",
    "        titulo = soup.find_all('p', {'class': 'text-title'})[0].text\n",
    "    except:\n",
    "        titulo = 'No conseguido'\n",
    "    \n",
    "    try:\n",
    "        presentacion = soup.find_all('p', {'class': 'text-description'})[0].text.strip()\n",
    "    except:\n",
    "        presentacion = 'No conseguido'\n",
    "    \n",
    "    try:\n",
    "        precio_final = float(''.join([x for x in soup.find_all('span', {'class': 'text-price'})[0].text.strip() if x.isdigit()]))\n",
    "    except:\n",
    "        precio_final = 0\n",
    "    \n",
    "    try:\n",
    "        precio_tachado = float(''.join([x for x in soup.find_all('span', {'class': 'text-offer-price'})[0].text.strip() if x.isdigit()]))\n",
    "    except:\n",
    "        precio_tachado = 0\n",
    "    \n",
    "    try:\n",
    "        precio_por = soup.find_all('p', {'class': 'text-price-unit'})[0].text.strip()\n",
    "    except:\n",
    "        precio_por = 0\n",
    "    \n",
    "    try:\n",
    "        calificacion = float(soup.find_all('div', {'class': 'bv_averageRating_component_container'})[0].text)\n",
    "        numero_calificaciones = int(soup.find_all('div', {'class': 'bv_numReviews_component_container'})[0].text.replace('(','').replace(')',''))\n",
    "    except:\n",
    "        calificacion = 0\n",
    "        numero_calificaciones = 0\n",
    "    \n",
    "    try:\n",
    "        tiempo_entrega = soup.find_all('div', {'class': 'calendar hide-r'})[0].text\n",
    "    except:\n",
    "        tiempo_entrega = 'N/A'\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'fecha_scraping': dt.now().strftime('%Y-%m-%d'),\n",
    "            'hora_scraping': dt.now().strftime('%H:%M:%S'),\n",
    "            'titulo': titulo,\n",
    "            'presentacion': presentacion,\n",
    "            'precio_por': precio_por,\n",
    "            'calificacion': calificacion,\n",
    "            'numero_calificaciones': numero_calificaciones,\n",
    "            'tiempo_entrega': tiempo_entrega,\n",
    "            'precio_tachado': precio_tachado,\n",
    "            'precio_final': precio_final\n",
    "        },\n",
    "        orient = 'index'\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def carga_producto_postgres(fila):\n",
    "    db = conn2('fsalinas', False)\n",
    "    sql = f'''\n",
    "    INSERT INTO web_scraping.drog_farmatodo (url_producto, fecha_scraping, hora_scraping, titulo, presentacion, precio_por, calificacion, numero_calificaciones, tiempo_entrega, precio_tachado, precio_final)\n",
    "    SELECT *\n",
    "    FROM (values{fila}) as s(url_producto, fecha_scraping, hora_scraping, titulo, presentacion, precio_por, calificacion, numero_calificaciones, tiempo_entrega, precio_tachado, precio_final)\n",
    "    '''\n",
    "    print(str(RunDML(sql, db)[0])[:100], end='\\r')\n",
    "    db.close()\n",
    "\n",
    "def recorre_url_cat(navegador, url):\n",
    "    navegador.get(url)\n",
    "    time.sleep(5)\n",
    "    try:\n",
    "        boton_cargar_mas = navegador.find_element_by_id('group-view-load-more')\n",
    "        while True:\n",
    "            try:\n",
    "                boton_cargar_mas.send_keys(Keys.PAGE_DOWN)\n",
    "                boton_cargar_mas.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                break\n",
    "    except:\n",
    "        'continue'\n",
    "    \n",
    "    time.sleep(5)\n",
    "    l_soup = BeautifulSoup(navegador.page_source, 'html.parser').find_all('div', {'class': 'card-ftd'})\n",
    "\n",
    "    df_concat = pd.concat([obtiene_data_producto(soup) for soup in l_soup], axis=1, ignore_index=True).T.drop_duplicates(subset='url').reset_index().iloc[:,1:]\n",
    "    return df_concat\n",
    "\n",
    "inicio = dt.now()\n",
    "print(f'{inicio.strftime(\"%H:%M:%S\")}: Inicio Proceso')\n",
    "for url in cat_urls:\n",
    "    try:\n",
    "        tdf = recorre_url_cat(navegador, url_principal + url)\n",
    "        for x in tdf.index:\n",
    "            try:\n",
    "                carga_producto_postgres(tuple(tdf.iloc[x:x+1,:].values.tolist()[0]))\n",
    "            except:\n",
    "                print('Producto no cargado...', end='\\r')\n",
    "    except:\n",
    "        print('Producto no cargado...', end='\\r')\n",
    "\n",
    "print(f'{dt.now().strftime(\"%H:%M:%S\")}: Fin Proceso')\n",
    "print(f'Duración: {dt.now()-inicio}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
